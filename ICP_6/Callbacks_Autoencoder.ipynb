{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wR1nmVDaEWs"
      },
      "source": [
        "Apply the following callback functions on autoencoder.\n",
        "\n",
        "*   TerminateOnNaN\n",
        "*   EarlyStopping\n",
        "*   ModelCheckpoint\n",
        "*   ReduceLROnPlateau\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Pkreop0hWZpQ"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Define the input and encoding layers\n",
        "input_dim = 784  # Example for flattened 28x28 images (like MNIST)\n",
        "encoding_dim = 64  # Dimensionality of latent space\n",
        "\n",
        "input_img = Input(shape=(input_dim,))\n",
        "encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
        "decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
        "\n",
        "# Build the autoencoder model\n",
        "autoencoder = Model(input_img, decoded)\n",
        "\n",
        "# Compile the autoencoder\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "e0AZDZu8W_QY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss',\n",
        "                               patience=5,  # Number of epochs with no improvement after which training will be stopped\n",
        "                               restore_best_weights=True)  # Restores model to best weights with the lowest validation loss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zjwPt06BXDy1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'x_train' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming x_train and x_test are your training and test datasets\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m autoencoder\u001b[38;5;241m.\u001b[39mfit(\u001b[43mx_train\u001b[49m, x_train,  \u001b[38;5;66;03m# For autoencoders, input and output are the same\u001b[39;00m\n\u001b[0;32m      3\u001b[0m                 epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# Set a high number of epochs\u001b[39;00m\n\u001b[0;32m      4\u001b[0m                 batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[0;32m      5\u001b[0m                 shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      6\u001b[0m                 validation_data\u001b[38;5;241m=\u001b[39m(x_test, x_test),\n\u001b[0;32m      7\u001b[0m                 callbacks\u001b[38;5;241m=\u001b[39m[early_stopping])  \u001b[38;5;66;03m# Add the early stopping callback\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
          ]
        }
      ],
      "source": [
        "# Assuming x_train and x_test are your training and test datasets\n",
        "autoencoder.fit(x_train, x_train,  # For autoencoders, input and output are the same\n",
        "                epochs=100,  # Set a high number of epochs\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[early_stopping])  # Add the early stopping callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWNHcuLPXEhg"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import TerminateOnNaN\n",
        "\n",
        "# Define the TerminateOnNaN callback\n",
        "terminate_on_nan = TerminateOnNaN()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdPH-DZGXkma"
      },
      "outputs": [],
      "source": [
        "# Assuming x_train and x_test are your training and validation datasets\n",
        "autoencoder.fit(x_train, x_train,  # For autoencoders, input and output are the same\n",
        "                epochs=100,  # Set the number of epochs\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[terminate_on_nan])  # Add the TerminateOnNaN callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bsG5OHeXn1-"
      },
      "outputs": [],
      "source": [
        "# EarlyStopping callback to stop training if validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Training with multiple callbacks\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[terminate_on_nan, early_stopping])  # Using both callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tPATuVKWXu6X"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define the ModelCheckpoint callback\n",
        "checkpoint = ModelCheckpoint(filepath='autoencoder_best.h5',  # File path to save the model\n",
        "                             monitor='val_loss',  # Metric to monitor\n",
        "                             save_best_only=True,  # Save only the best model (based on the monitored metric)\n",
        "                             mode='min',  # Minimize the monitored metric (e.g., validation loss)\n",
        "                             save_weights_only=False,  # Save the entire model (set to True to save only weights)\n",
        "                             verbose=1)  # Print a message when saving the model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9A48Z_0YyIQ"
      },
      "outputs": [],
      "source": [
        "# Assuming x_train and x_test are your training and validation datasets\n",
        "autoencoder.fit(x_train, x_train,  # For autoencoders, input and output are the same\n",
        "                epochs=50,  # Number of epochs\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),  # Validation data\n",
        "                callbacks=[checkpoint])  # Add the ModelCheckpoint callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8BXyUP9Y2sk"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, TerminateOnNaN\n",
        "\n",
        "# EarlyStopping callback to stop training if validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# TerminateOnNaN callback to stop training if the loss becomes NaN\n",
        "terminate_on_nan = TerminateOnNaN()\n",
        "\n",
        "# Training with multiple callbacks\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,  # You can set a high number of epochs\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[checkpoint, early_stopping, terminate_on_nan])  # Using multiple callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MKcjBAvY71K"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the entire model\n",
        "best_autoencoder = load_model('autoencoder_best.h5')\n",
        "\n",
        "# If you only saved the weights:\n",
        "# autoencoder.load_weights('autoencoder_best.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eReuE5DEZA39"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "# Define the ReduceLROnPlateau callback\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss',  # Metric to monitor\n",
        "                              factor=0.5,  # Factor by which the learning rate will be reduced (new_lr = lr * factor)\n",
        "                              patience=3,  # Number of epochs with no improvement after which learning rate will be reduced\n",
        "                              min_lr=1e-6,  # Lower bound for the learning rate\n",
        "                              verbose=1)  # Print message when the learning rate is reduced\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcF0CObWZnck"
      },
      "outputs": [],
      "source": [
        "# Assuming x_train and x_test are your training and validation datasets\n",
        "autoencoder.fit(x_train, x_train,  # For autoencoders, input and output are the same\n",
        "                epochs=50,  # Number of epochs\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),  # Validation data\n",
        "                callbacks=[reduce_lr])  # Add the ReduceLROnPlateau callback\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EhaqM7xZq0o"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, TerminateOnNaN\n",
        "\n",
        "# EarlyStopping callback to stop training if validation loss stops improving\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# ModelCheckpoint callback to save the best model based on validation loss\n",
        "checkpoint = ModelCheckpoint(filepath='autoencoder_best.h5', monitor='val_loss', save_best_only=True, verbose=1)\n",
        "\n",
        "# TerminateOnNaN callback to stop training if the loss becomes NaN\n",
        "terminate_on_nan = TerminateOnNaN()\n",
        "\n",
        "# Training with multiple callbacks\n",
        "autoencoder.fit(x_train, x_train,\n",
        "                epochs=100,  # You can set a high number of epochs\n",
        "                batch_size=256,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test),\n",
        "                callbacks=[reduce_lr, early_stopping, checkpoint, terminate_on_nan])  # Using multiple callbacks\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dw-fHUqZuDk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
